{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514ba66b492ad96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T13:27:34.633616Z",
     "start_time": "2024-10-15T13:27:34.563451Z"
    }
   },
   "outputs": [],
   "source": [
    "%run base.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03530396327ad21",
   "metadata": {},
   "source": [
    "# Project Demo\n",
    "In this notebook we want to verify the total annual revenue of Volkswagen in 2024. I have uploaded an (intentionally wrong) dataset to the test instance of Dataland: https://test.dataland.com/companies/2dae3e2d-3eb9-4f0d-84ab-ad740cd8a439/frameworks/eutaxonomy-non-financials/f0ee2e62-9349-4451-a718-16a2bf20164d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad732118e7df09",
   "metadata": {},
   "source": [
    "## 1. Load the data from Dataland\n",
    "To verify or reject the claim we first need to load the data from Dataland. We will use the Dataland API to do so. Datasets carry unique identifiers. These can e.g., be found in the URL of the dataset. The Dataland URLs follow the pattern https://dataland.com/companies/{company_id}/frameworks/{framework_id}/{data_id}. In our case the data_id is \"f0ee2e62-9349-4451-a718-16a2bf20164d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9cc28d1ae00a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:19:54.429540Z",
     "start_time": "2024-10-15T12:19:53.017738Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataland_qa_lab.utils import config\n",
    "\n",
    "conf = config.get_config()\n",
    "dataland_client = conf.dataland_client\n",
    "\n",
    "data_id = \"f0ee2e62-9349-4451-a718-16a2bf20164d\"\n",
    "\n",
    "dataset = dataland_client.eu_taxonomy_nf_api.get_company_associated_eutaxonomy_non_financials_data(data_id=data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c463661f4ddbf",
   "metadata": {},
   "source": [
    "Dataset may contain hundreds of records. For simplicity, we removed all datapoints except for the total revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c92ffb0c986007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:19:55.106128Z",
     "start_time": "2024-10-15T12:19:55.102613Z"
    }
   },
   "outputs": [],
   "source": [
    "revenue_datapoint = dataset.data.revenue.total_amount\n",
    "revenue_datapoint.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a908bd5775e83",
   "metadata": {},
   "source": [
    "To verify the datapoint we need to check whether the specified revenue matches the revenue of the underlying datasource. On Dataland data-sources (i.e., PDFs) are identified by their SHA-256 hash. We can use the Dataland API to download the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78d20eacb265fa",
   "metadata": {},
   "source": [
    "# 2. Load the data-source from Dataland and convert it to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad984ea8a5e643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:19:59.685765Z",
     "start_time": "2024-10-15T12:19:57.353470Z"
    }
   },
   "outputs": [],
   "source": [
    "document_bytes = dataland_client.documents_api.get_document(revenue_datapoint.data_source.file_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445603b0dc679ab1",
   "metadata": {},
   "source": [
    "The raw PDF is not of much use to us. We need to extract the text from the PDF to process it further (although you are welcome to experiment with using vision-enabled LLMs instead). Extracting text from PDFs is very challenging. Dealing with tables is especially troublesome. Take a look at https://pypdf.readthedocs.io/en/latest/user/extract-text.html#why-text-extraction-is-hard if you are curious. Due to these challenges, we'll use Azure Document Intelligence to extract the text from the PDF. \n",
    "\n",
    "The Document Intelligence API charges per page. The entire document is ~400 pages long. To save costs we will only analyze the page containing the revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42edb07dec1452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:00.663597Z",
     "start_time": "2024-10-15T12:20:00.381386Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pypdf\n",
    "\n",
    "full_document_byte_stream = io.BytesIO(document_bytes)\n",
    "full_pdf = pypdf.PdfReader(full_document_byte_stream)\n",
    "\n",
    "partial_document_byte_stream = io.BytesIO()\n",
    "partial_pdf = pypdf.PdfWriter()\n",
    "\n",
    "partial_pdf.add_page(full_pdf.get_page(int(revenue_datapoint.data_source.page) - 1))  # Correct for 0 offset\n",
    "partial_pdf.write(partial_document_byte_stream)\n",
    "partial_document_byte_stream.seek(0)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dff99b4c0b721",
   "metadata": {},
   "source": [
    "Now we can use the Azure Document Intelligence API to extract the text from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800dddb714a240f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:06.330186Z",
     "start_time": "2024-10-15T12:20:01.792543Z"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult, DocumentContentFormat\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "docintel_cred = AzureKeyCredential(conf.azure_docintel_api_key)\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=conf.azure_docintel_endpoint, credential=docintel_cred\n",
    ")\n",
    "\n",
    "poller = document_intelligence_client.begin_analyze_document(\n",
    "    \"prebuilt-layout\",\n",
    "    body=partial_document_byte_stream,\n",
    "    content_type=\"application/octet-stream\",\n",
    "    output_content_format=DocumentContentFormat.MARKDOWN,\n",
    ")\n",
    "result: AnalyzeResult = poller.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404856905e81fe2",
   "metadata": {},
   "source": [
    "The result is a markdown document. We can display it directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b074b5ea309a400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:07.608473Z",
     "start_time": "2024-10-15T12:20:07.605542Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e02585ca4c638",
   "metadata": {},
   "source": [
    "From the table, we can observe that the total revenue is 324,656 € Million. Just from this example you can see that extracting such data is very challenging. You e.g., have to notice that all values in the table are given in € Million."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b07870e89f4b64",
   "metadata": {},
   "source": [
    "# 3. Verify the claim using GPT-5\n",
    "To verify the claim we will use the GPT-5 model. We provide the model with the text extracted from the PDF and ask it to extract the total revenue. Afterward, we can compare the extracted value to the claimed value. To achieve this, we need to build a prompt that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38af1838d6e618e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:12.156424Z",
     "start_time": "2024-10-15T12:20:10.761799Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=conf.azure_openai_api_key, api_version=\"2024-07-01-preview\", azure_endpoint=conf.azure_openai_endpoint\n",
    ")\n",
    "\n",
    "deployment_name = \"gpt-5\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI research Agent. As the agent, you answer questions briefly, succinctly, and factually.\n",
    "Always justify you answer.\n",
    "\n",
    "# Safety\n",
    "- You **should always** reference factual statements to search results based on [relevant documents]\n",
    "- Search results based on [relevant documents] may be incomplete or irrelevant. You do not make assumptions\n",
    "  on the search results beyond strictly what's returned.\n",
    "- If the search results based on [relevant documents] do not contain sufficient information to answer user\n",
    "  message completely, you respond using the tool 'cannot_answer_question'\n",
    "- Your responses should avoid being vague, controversial or off-topic.\n",
    "\n",
    "# Task\n",
    "Given the information from the [relevant documents], what is the total revenue of Volkswagen in 2024?\n",
    "\n",
    "# Relevant Documents\n",
    "{result.content}\n",
    "\"\"\"\n",
    "\n",
    "initial_openai_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "initial_openai_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4795e824f68df4c",
   "metadata": {},
   "source": [
    "We can see that the model has answered the question correctly. However, the response is not given in a structured way. We can use the tool calling feature of OpenAI to force the model to provide a structured response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39c2c36d5de591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:15.789800Z",
     "start_time": "2024-10-15T12:20:14.992377Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_openai_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ],\n",
    "    tool_choice=\"required\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"requested_information_precisely_found_in_relevant_documents\",\n",
    "                \"description\": \"Submit the requested information. \"\n",
    "                \"Use this function when the information is precisely stated in the relevant documents. \",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"answer_value\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"The precise answer to the imposed questionwithout any thousand separators.\",\n",
    "                        },\n",
    "                        \"answer_currency\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The currency of the answer (e.g., EUR, USD)\",\n",
    "                        },\n",
    "                        \"justification\": {\"type\": \"string\", \"description\": \"The justification for the answer\"},\n",
    "                    },\n",
    "                    \"required\": [\"answer_value\", \"answer_currency\", \"justification\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "tool_call = updated_openai_response.choices[0].message.tool_calls[0].function\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19484e9c5c48cb49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:17.085262Z",
     "start_time": "2024-10-15T12:20:17.080791Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parsed_tool_arguments = json.loads(tool_call.arguments)\n",
    "extracted_revenue = parsed_tool_arguments[\"answer_value\"]\n",
    "extracted_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f2cb2770178c3",
   "metadata": {},
   "source": [
    "This looks very promising and works in this very simple scenario. However, be aware that LLMs are not good at performing calculations. This is a problem you'll likely need to tackle later ;).\n",
    "Additionally, LLMs can also make a lot of other mistakes. However, in this case, it worked well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185182790a583c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:18.772783Z",
     "start_time": "2024-10-15T12:20:18.770174Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original Value: \\t{revenue_datapoint.value}\")\n",
    "print(f\"Extracted Value: \\t{extracted_revenue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867964f5e18b392",
   "metadata": {},
   "source": [
    "We can directly see that the values do not align. Therefore, the claim is incorrect. We report this information back to the data provider by creating a so-called QA Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850c66f1c4439d7",
   "metadata": {},
   "source": [
    "# 4. Creating and submitting a QA Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b44332f780e102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T12:20:23.728206Z",
     "start_time": "2024-10-15T12:20:22.723009Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataland_qa.models.currency_data_point import CurrencyDataPoint\n",
    "from dataland_qa.models.eutaxonomy_non_financials_data import EutaxonomyNonFinancialsData\n",
    "from dataland_qa.models.eutaxonomy_non_financials_revenue import EutaxonomyNonFinancialsRevenue\n",
    "from dataland_qa.models.extended_document_reference import ExtendedDocumentReference\n",
    "from dataland_qa.models.qa_report_data_point_currency_data_point import QaReportDataPointCurrencyDataPoint\n",
    "from dataland_qa.models.qa_report_data_point_verdict import QaReportDataPointVerdict\n",
    "\n",
    "selected_qa_report = EutaxonomyNonFinancialsData(\n",
    "    revenue=EutaxonomyNonFinancialsRevenue(\n",
    "        totalAmount=QaReportDataPointCurrencyDataPoint(\n",
    "            comment=\"The total revenue is incorrect. The correct value is 250200000000 €\",\n",
    "            verdict=QaReportDataPointVerdict.QAREJECTED,\n",
    "            correctedData=CurrencyDataPoint(\n",
    "                value=extracted_revenue,\n",
    "                quality=\"Reported\",\n",
    "                comment=parsed_tool_arguments[\"justification\"],\n",
    "                currency=parsed_tool_arguments[\"answer_currency\"],\n",
    "                dataSource=ExtendedDocumentReference.from_dict(revenue_datapoint.data_source.model_dump(by_alias=True)),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "dataland_client.eu_taxonomy_nf_qa_api.post_eutaxonomy_non_financials_data_qa_report(data_id, selected_qa_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49741f84fbf740",
   "metadata": {},
   "source": [
    "The created QA Report is now available to the data provider. They can review the report and adapt their data if necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalandqalab-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
